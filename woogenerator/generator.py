"""
Generator extracts data from a main and subordinate database of product
information, then synchronizes that information on several layers.
In a typical configuration, the main is a spreadsheet of product information
in the generator format, which is a custom heirarchical representation of
product and category data designed for minimal redundancy.
The generator format is a hybrid of a tree structure with a flat-file table.
Product titles and SKUs are generated by concatenating the name and code
fragments of a product's ancestors in a tree.

These are the steps that generator uses to synchronize products:
 - The main information is optionally downloaded from Google Drive if the
 `--download-main` flag is set, or it is read from local csv files.
 - Main information is parsed into a main `CSVParse` object which contains
 several `Import` objects which correspond to products, variations, images,
 categories and other objects within the database.
 - The main information is exported as a set of CSV file which is compatible
 with the WooCommerce Product CSV Import Suite plugin. At this point the
 program can be terminated if the `--main-and-quit` command line flag is set.
 - Subordinate product information is optionally downloaded from the the selected API
 and parsed in a similar way to main if the `--download-subordinate` flag is set,
 otherwise it is parsed from local json files
 - The objectes from these parser objects are matched with each other into a
 collection of `match` objects which need special attention when the `matcher`
 objects encounter duplicates. Matching has to be done separately for images,
 categories, products and product variations in this order since each of these
 layers are inter-dependent.
 - The `match` objects are analysed and a collection of `sync_update` objects
 are generated which describe how to update the databases so that they match.
 This process is referred to as merging.
 - Before any updates are done, some HTML reports are generated and the user
 is asked for confirmation before continuing.
 - The updates are then carried out on the subordinate database using a `sync_client`
 for each layer.

"""

from __future__ import absolute_import, print_function

import io
import itertools
import os
import shutil
import sys
import time
import traceback
import webbrowser
import zipfile
from collections import OrderedDict
from pprint import pformat, pprint

from exitstatus import ExitStatus
from six import string_types

from .images import process_images
from .matching import (CategoryMatcher, CategoryTitleMatcher, ImageMatcher,
                       Match, MatchList, ProductMatcher, StrictImageMatcher,
                       VariationMatcher)
from .namespace.core import (MatchNamespace, ParserNamespace, ResultsNamespace,
                             UpdateNamespace)
from .namespace.prod import SettingsNamespaceProd
from .parsing.dyn import CsvParseDyn
from .parsing.special import CsvParseSpecial
from .utils import (ProgressCounter, Registrar, SanitationUtils, SeqUtils,
                    TimeUtils)
from .utils.reporter import (ReporterNamespace, do_cat_sync_gruop,
                             do_category_matches_group, do_delta_group,
                             do_failures_group, do_img_sync_group,
                             do_main_summary_group, do_matches_group,
                             do_matches_summary_group, do_post_summary_group,
                             do_successes_group, do_sync_group,
                             do_var_sync_group, do_variation_matches_group)


# to run full sync test on TT
"""
python -m woogenerator.generator \
    --testmode --schema "TT" \
    --local-work-dir '/Users/derwent/Documents/woogenerator/' \
    --download-main --download-subordinate \
    --do-categories --do-images --do-specials \
    --do-sync --update-subordinate --do-problematic --auto-create-new \
    --ask-before-update \
    -vvv --debug-trace
"""

# To run full sync test on VT
"""
python -m woogenerator.generator \
    --testmode --schema "VT" \
    --local-work-dir '/Users/derwent/Documents/woogenerator/' \
    --local-test-config 'generator_config_test_vt.yaml' \
    --download-main --download-subordinate \
    --do-categories --do-specials \
    --do-images --do-resize-images \
    --do-sync --update-subordinate --do-problematic --auto-create-new \
    --ask-before-update \
    -vvv --debug-trace
"""

# TT just main and quit
"""
python -m woogenerator.generator \
    --testmode --schema "TT" \
    --local-work-dir '/Users/derwent/Documents/woogenerator/' \
    --download-main --download-subordinate \
    --do-categories --do-images --do-variations --main-and-quit \
    --do-specials --specials-mode 'auto_next' \
    -vvv --debug-trace
"""

# VT just main and quit
"""
python -m woogenerator.generator \
    --testmode --schema "VT" \
    --local-work-dir '/Users/derwent/Documents/woogenerator/' \
    --local-test-config 'generator_config_test_vt.yaml' \
    --download-main --download-subordinate \
    --do-categories --do-images --do-variations --main-and-quit \
    --do-specials --specials-mode 'all_future' \
    -vvv --debug-trace
"""


def timediff(settings):
    """Return time elapsed since start."""
    return time.time() - settings.start_time


def check_warnings(settings):
    """
    Check if there have been any errors or warnings registered in Registrar.

    Raise approprriate exceptions if needed
    """
    if Registrar.errors:
        print("there were some urgent errors "
              "that need to be reviewed before continuing")
        Registrar.print_message_dict(0)
        usr_prompt_continue(settings)
        if Registrar.DEBUG_TRACE:
            import pudb
            pudb.set_trace()

    elif Registrar.warnings:
        print("there were some warnings that should be reviewed")
        Registrar.print_message_dict(1)


def populate_main_parsers(parsers, settings):
    """Create and populates the various parsers."""
    Registrar.register_message('schema: %s, woo_schemas: %s' %
                               (settings.schema, settings.woo_schemas))

    parsers.dyn = CsvParseDyn()
    parsers.special = CsvParseSpecial()

    if Registrar.DEBUG_GEN:
        Registrar.register_message("main_download_client_args: %s" %
                                   settings.main_download_client_args)

    with settings.main_download_client_class(
            **settings.main_download_client_args) as client:

        if settings.schema_is_woo:
            if settings.do_dyns:
                Registrar.register_message("analysing dprc rules")
                client.analyse_remote(
                    parsers.dyn,
                    data_path=settings.dprc_path,
                    gid=settings.dprc_gid)
                settings.dprc_rules = parsers.dyn.taxos

                Registrar.register_message("analysing dprp rules")
                parsers.dyn.clear_transients()
                client.analyse_remote(
                    parsers.dyn,
                    data_path=settings.dprp_path,
                    gid=settings.dprp_gid)
                settings.dprp_rules = parsers.dyn.taxos

            if settings.do_specials:
                Registrar.register_message("analysing specials")
                client.analyse_remote(
                    parsers.special,
                    data_path=settings.specials_path,
                    gid=settings.spec_gid)
                if Registrar.DEBUG_SPECIAL:
                    Registrar.register_message(
                        "all specials: %s" % parsers.special.tabulate())

                settings.special_rules = parsers.special.rules

                settings.current_special_groups = \
                    parsers.special.determine_current_spec_grps(
                        specials_mode=settings.specials_mode,
                        current_special=settings.current_special
                    )

                if settings.current_special_groups:
                    Registrar.register_message(
                        "current_special_groups: \n%s" %
                        (parsers.special.taxo_container.container(
                            settings.current_special_groups).tabulate()))
                else:
                    Registrar.register_warning(
                        ("No special groups were found, "
                         "here are the latest specials: \n%s") %
                        (parsers.special.taxo_container.container(
                            parsers.special.last_5()).tabulate()))

        main_parser_args = settings.main_parser_args

        main_mod_dt = TimeUtils.current_datetime()
        if os.path.exists(settings.main_path):
            main_mod_ts = max(
                os.path.getmtime(settings.main_path),
                os.path.getctime(settings.main_path))
            main_mod_dt = TimeUtils.inform_datetime(
                TimeUtils.timestamp2datetime(main_mod_ts),
                TimeUtils._local_tz)
        elif hasattr(client, 'get_gm_modtime'):
            main_mod_dt = TimeUtils.localize_datetime(
                client.get_gm_modtime(settings.gen_gid), TimeUtils._local_tz)

        main_parser_args['defaults'].update({
            'modified_local':
            main_mod_dt,
            'modified_gmt':
            TimeUtils.localize_datetime(main_mod_dt, TimeUtils.utc_tz)
        })

        parsers.main = settings.main_parser_class(**main_parser_args)

        Registrar.register_progress("analysing main product data")

        analysis_kwargs = {
            'data_path': settings.main_path,
            'gid': settings.gen_gid,
            'limit': settings['main_parse_limit']
        }
        if Registrar.DEBUG_PARSER:
            Registrar.register_message("analysis_kwargs: %s" % analysis_kwargs)

        client.analyse_remote(parsers.main, **analysis_kwargs)

        if Registrar.DEBUG_PARSER and hasattr(parsers.main,
                                              'categories_name'):
            for category_name, category_list in getattr(
                    parsers.main, 'categories_name').items():
                if len(category_list) < 2:
                    continue
                if SeqUtils.check_equal([
                    category.namesum for category in category_list
                ]):  # yapf: disable
                    continue
                Registrar.register_warning(
                    "bad category: %50s | %d | %s" % (category_name[:50],
                                                      len(category_list),
                                                      str(category_list)))

        return parsers


def populate_subordinate_parsers(parsers, settings):
    """Populate the parsers for data from the subordinate database."""

    parsers.subordinate = settings.subordinate_parser_class(**settings.subordinate_parser_args)

    subordinate_client_class = settings.subordinate_download_client_class
    subordinate_client_args = settings.subordinate_download_client_args
    subordinate_var_client_class = settings.subordinate_var_sync_client_class
    subordinate_var_client_args = settings.subordinate_var_sync_client_args

    # with ProdSyncClientWC(settings['subordinate_wp_api_params']) as client:

    if settings.schema_is_woo and settings.do_categories:
        Registrar.register_progress("analysing API category data")

        cat_sync_client_class = settings.subordinate_cat_sync_client_class
        cat_sync_client_args = settings.subordinate_cat_sync_client_args

        with cat_sync_client_class(**cat_sync_client_args) as client:
            client.analyse_remote_categories(
                parsers.subordinate, data_path=settings.subordinate_cat_path)

    # TODO: ignore products which are post_status = trash

    with subordinate_client_class(**subordinate_client_args) as client:

        Registrar.register_progress("analysing API product data")

        client.analyse_remote(
            parsers.subordinate,
            data_path=settings.subordinate_path,
        )

        if settings.do_variations:

            Registrar.register_progress("analysing API variation data")

            var_client = subordinate_var_client_class(**subordinate_var_client_args)

            progress_counter = ProgressCounter(
                total=len(parsers.subordinate.variations),
                items_plural='variations',
                verb_past='analysed'
            )

            for prod in parsers.subordinate.products.values():
                if not getattr(prod, 'is_variable', None):
                    continue
                parent_id = prod.api_id
                var_client.analyse_remote_variations(
                    parsers.subordinate,
                    parent_pkey=parent_id,
                    data_path=settings.get_subordinate_var_path(parent_id),
                    progress_counter=progress_counter
                )

    if settings.schema_is_woo and settings.do_images:
        Registrar.register_progress("analysing API image data")
        img_client_class = settings.subordinate_img_sync_client_class
        img_client_args = settings.subordinate_img_sync_client_args

        with img_client_class(**img_client_args) as client:
            client.analyse_remote_imgs(
                parsers.subordinate,
                data_path=settings.subordinate_img_path,
                skip_unattached_images=settings.skip_unattached_images)

    if Registrar.DEBUG_CLIENT:
        container = settings.subordinate_parser_class.product_container.container
        prod_list = container(parsers.subordinate.products.values())
        Registrar.register_message("Products: \n%s" % prod_list.tabulate())

    return parsers


def export_categories(settings, parser, csv_file, export_target):
    # category_cols = settings.coldata_class_cat.get_col_data_native(
    #     'write', target=export_target)
    category_col_names = settings.coldata_class_cat.get_col_values_native(
        'path', target=export_target)
    for col in settings.exclude_cols_cat:
        if col in category_col_names:
            del category_col_names[col]
    category_container = \
        settings.main_parser_class.category_container.container
    category_list = category_container([
        category for category in parser.categories.values() if category.members
    ])
    category_list.export_items(
        csv_file, category_col_names, coldata_target=export_target)


def export_main_parser(settings, parsers):
    """Export key information from main parser to csv."""
    Registrar.register_progress("Exporting Main info to disk")

    export_target = 'wc-csv'

    # Create output directory if not exist
    try:
        os.makedirs(settings.out_dir_full)
    except OSError:
        pass

    product_colnames = settings.coldata_class.get_col_values_native(
        'path', target=export_target)

    for col in settings.exclude_cols:
        if col in product_colnames:
            del product_colnames[col]

    extra_colnames = OrderedDict([('title_1', 'meta:title_1'),
                                  ('title_2', 'meta:title_2')])
    if settings.schema_is_woo and settings.do_attributes:
        extra_colnames = SeqUtils.combine_ordered_dicts(
            extra_colnames,
            settings.coldata_class.get_attribute_colnames_native(
                parsers.main.attributes, parsers.main.vattributes))
    product_colnames = SeqUtils.combine_ordered_dicts(product_colnames,
                                                      extra_colnames)

    container = parsers.main.product_container.container

    product_list = container(parsers.main.products.values())

    product_list.export_items(
        settings.fla_path,
        product_colnames,
        coldata_target=export_target,
        extra_colnames=extra_colnames)

    # TODO: stop exporting modified_gmt to spreadsheet

    if settings.schema_is_woo:
        # variations
        variation_container = \
            settings.main_parser_class.variation_container.container
        # variation_cols = \
        #     settings.coldata_class_var.get_col_data_native(
        #         'write', target='wc-csv')
        variation_col_names = \
            settings.coldata_class_var.get_col_values_native(
                'path', target=export_target)
        extra_variation_col_names = \
            settings.coldata_class_var.get_attribute_meta_colnames_native(
                parsers.main.vattributes
            )
        variation_col_names = SeqUtils.combine_ordered_dicts(
            variation_col_names, extra_variation_col_names)
        if settings.do_variations and parsers.main.variations:

            variation_list = variation_container(
                parsers.main.variations.values())
            variation_list.export_items(
                settings.flv_path,
                variation_col_names,
                coldata_target=export_target,
                extra_colnames=extra_variation_col_names)

            updated_variations = parsers.main.updated_variations.values()

            if updated_variations:
                updated_variations_list = variation_container(
                    updated_variations)
                updated_variations_list.export_items(
                    settings.flvu_path,
                    variation_col_names,
                    coldata_target=export_target,
                    extra_colnames=extra_variation_col_names)

        # categories
        if settings.do_categories and parsers.main.categories:
            export_categories(settings, parsers.main, settings.cat_path,
                              export_target)

        # specials
        if settings.do_specials and settings.current_special_id:
            special_products = parsers.main.onspecial_products.values()
            if special_products:
                special_product_list = container(special_products)
                special_product_list.export_items(
                    settings.fls_path,
                    product_colnames,
                    coldata_target=export_target,
                    extra_colnames=extra_colnames)
            special_variations = parsers.main.onspecial_variations.values()
            if special_variations:
                sp_variation_list = variation_container(special_variations)
                sp_variation_list.export_items(
                    settings.flvs_path,
                    variation_col_names,
                    coldata_target=export_target,
                    extra_colnames=extra_variation_col_names)

        updated_products = parsers.main.updated_products.values()
        if updated_products:
            updated_product_list = container(updated_products)
            updated_product_list.export_items(
                settings.flu_path,
                product_colnames,
                coldata_target=export_target,
                extra_colnames=extra_colnames)

            # TODO; updated variations

    Registrar.register_progress("CSV Files have been created.")


def cache_api_data(settings, parsers):
    """Export key information from subordinate parser to csv."""
    if not settings.download_subordinate:
        return
    if not settings.save_api_data:
        return

    Registrar.register_progress("Exporting Subordinate info to disk")
    container = settings.subordinate_parser_class.product_container.container
    product_list = container(parsers.subordinate.products.values())
    product_list.export_api_data(settings.subordinate_path)

    if settings.do_categories and parsers.subordinate.categories:
        category_container = \
            settings.subordinate_parser_class.category_container.container
        category_list = category_container(parsers.subordinate.categories.values())
        category_list.export_api_data(settings.subordinate_cat_path)

    if settings.do_images and parsers.subordinate.attachments:
        attachment_container = \
            settings.subordinate_parser_class.attachment_container.container
        image_list = attachment_container(parsers.subordinate.attachments.values())
        image_list.export_api_data(settings.subordinate_img_path)

    if settings.do_variations and parsers.subordinate.variations:
        variation_container = \
            settings.subordinate_parser_class.variation_container.container
        for prod in product_list:
            if not prod.is_variable:
                continue
            var_list = variation_container(prod.variations.values())
            if not var_list:
                continue
            var_list.export_api_data(settings.get_subordinate_var_path(prod.api_id))


def do_match_images(parsers, matches, settings):
    """
    Match images.

    Note: the aim of the game is to eventually populate each main
    attachment object with a subordinate api id, so either the main can be matched
    (in a duplicate match with 1 main, or a pure match)
    or it must be created in subordinate (a subordinateless match).
    """
    if Registrar.DEBUG_IMG:
        Registrar.register_message(
            "matching %d main attachments with %d subordinate attachments" % (len(
                parsers.main.attachments), len(parsers.subordinate.attachments)))

    matches.image = MatchNamespace(index_fn=StrictImageMatcher.image_index_fn)

    image_matcher = ImageMatcher()
    image_matcher.clear()
    main_imgs_attachments = OrderedDict(
        [(index, image) for index, image in parsers.main.attachments.items()
         if image.attaches.has_products_categories])
    subordinate_imgs_attachments = parsers.subordinate.attachments
    if settings.skip_unattached_images:
        subordinate_imgs_attachments = OrderedDict(
            [(index, image) for index, image in subordinate_imgs_attachments.items()
             if image.attaches.has_products_categories])

    image_matcher.process_registers(subordinate_imgs_attachments,
                                    main_imgs_attachments)

    matches.image.globals.add_matches(image_matcher.pure_matches)
    matches.image.mainless.add_matches(image_matcher.mainless_matches)
    matches.image.subordinateless.add_matches(image_matcher.subordinateless_matches)

    if Registrar.DEBUG_IMG:
        if image_matcher.pure_matches:
            Registrar.register_message("All Image matches:\n%s" % ('\n'.join(
                map(str, image_matcher.matches))))

    matches.image.valid += image_matcher.pure_matches

    if not image_matcher.duplicate_matches:
        return matches

    extra_valid_indices_m = set()
    extra_valid_indices_s = set()

    matches.image.duplicate['normalized_filename'] = MatchList(
        index_fn=(lambda img: img.normalized_filename)
    )
    matches.image.duplicate['file_basename'] = MatchList(
        index_fn=(lambda img: img.file_basename)
    )
    matches.image.duplicate['norm_title'] = MatchList(
        index_fn=(lambda img: img.norm_title)
    )
    matches.image.duplicate['attachee_skus'] = MatchList(
        index_fn=(lambda img: img.attachee_skus)
    )

    filename_duplicate_indices_m = {
        attachment.index for match in image_matcher.duplicate_matches
        for attachment in match.m_objects
    }

    filename_duplicate_indices_s = {
        attachment.index for match in image_matcher.duplicate_matches
        for attachment in match.s_objects
    }

    def process_extra_match(extra_match, duplicate_name='normalized_filename'):
        if extra_match.type == 'duplicate' and extra_match.m_len != 1:
            try:
                matches.image.duplicate[duplicate_name].add_matches(
                    [extra_match])
            except Exception as exc:
                pass
            return
        extra_valid_indices_m.update(
            [attachment.index for attachment in extra_match.m_objects])
        extra_valid_indices_s.update(
            [attachment.index for attachment in extra_match.s_objects])
        if extra_match.type == 'pure':
            matches.image.valid += [extra_match]
        elif extra_match.type == 'mainless':
            try:
                matches.image.mainless.add_matches([extra_match])
            except AssertionError as exc:
                Registrar.register_warning(exc)
        elif extra_match.type == 'subordinateless':
            try:
                matches.image.subordinateless.add_matches([extra_match])
            except AssertionError as exc:
                Registrar.register_warning(exc)
        elif extra_match.type == 'duplicate' and extra_match.m_len == 1:
            matches.image.valid += [extra_match]

    for match in image_matcher.duplicate_matches:
        if Registrar.DEBUG_IMG or Registrar.DEBUG_TRACE:
            Registrar.register_message(
                "analysing duplicate match:\n%s" % match.tabulate())
        # why this mess? Well, let me explain.
        # Let's say we have a file 'placeholder.png' that has been uploaded
        #   to the server multiple times, each of these files will be called
        #   placeholder-1.png etc. normalized filename gets rid of this suffix
        #   but it doesn't always work. Lets say you have a file CVEXG-100.png
        #   this file will normalize to CVEXG.png, so that's why we have to
        #   first try to sub-match on file basename first, then we sub-match on
        #   the SKUs of all the products that are attached to the file

        remaining_match = match.__class__()
        remaining_match.consume(match)

        for allow_dup in range(2):
            for sub_matching, dup_list in [
                ('find_file_basename_matches', 'file_basename'),
                ('find_attachee_sku_matches', 'attachee_skus'),
                ('find_norm_title_matches', 'title_matches'),
            ]:  # yapf: disable
                sub_matches = getattr(
                    image_matcher, sub_matching)(remaining_match)
                remaining_match = remaining_match.__class__()
                for key, sub_match in sub_matches.items():
                    if Registrar.DEBUG_IMG or Registrar.DEBUG_TRACE:
                        Registrar.register_message(
                            "%s sub match %s is %s:\n%s" % (
                                sub_matching, key, sub_match.type,
                                sub_match.tabulate()))
                    if (
                        sub_match.type == 'pure'
                        or (
                            allow_dup and sub_match.type == 'duplicate'
                            and sub_match.m_len == 1
                        )
                    ):  # yapf: disable
                        process_extra_match(sub_match, dup_list)
                    else:
                        remaining_match.consume(sub_match)
                if remaining_match.type == 'empty':
                    break
                if Registrar.DEBUG_IMG or Registrar.DEBUG_TRACE:
                    Registrar.register_message(
                        "%s remaining match is %s:\n%s" % (
                            sub_matching, remaining_match.type,
                            remaining_match.tabulate()))
                if remaining_match.type == 'duplicate':
                    continue
                process_extra_match(remaining_match)
                remaining_match = remaining_match.__class__()
                break

        if remaining_match.type != 'empty':
            exc = UserWarning(("Could not match images\n%s") % (
                remaining_match.tabulate()))
            Registrar.register_warning(exc)

            process_extra_match(remaining_match, 'normalized_filename')

    try:
        assert \
            extra_valid_indices_m.issuperset(filename_duplicate_indices_m), \
            (
                "all main indices from filename duplicates should be "
                "contained in extra attachee match indices:\n"
                "filename:\n"
                "%s\n"
                "attachee_indices:\n"
                "%s\n"
                "difference:\n"
                "%s\n"
                "What this means (in English):\n"
                "When one image filename is associated with multiple "
                "products, it can be difficult to determine how to match "
                "these images on the server. Try making a copy of each of "
                "the 'difference' files for each product they are assigned to "
                "and updating the information in the spreadsheet and then "
                "re-run the sync."
            ) % (
                filename_duplicate_indices_m,
                extra_valid_indices_m,
                filename_duplicate_indices_m - extra_valid_indices_m
            )
        assert \
            extra_valid_indices_s.issuperset(filename_duplicate_indices_s), \
            (
                "all subordinate indices from filename duplicates should be "
                "contained in extra attachee match indices:\nfilename:\n"
                "%s\nattachee_indices:\n%s"
            ) % (
                filename_duplicate_indices_s,
                extra_valid_indices_s
            )
    except AssertionError as exc:
        warn = RuntimeWarning((
                "could not match all images, "
                "Some images may not sync until you have resolved this.\n"
                "%s\n"
                "%s"
            ) % ("\n".join([
                "%s:\n%s" % (key, dup_matches.tabulate())
                for key, dup_matches in matches.image.duplicate.items()
            ]), str(exc)))
        Registrar.register_warning(warn)

    return matches


def do_match_categories(parsers, matches, settings):
    """
    Analyse the main and subordinate categories for matches.

    Categorising these matches into either
     - mainless = a category exists in subordinate that doesn't exist in main
     - subordinateless = a category exists in main that doesn't exist in subordinate
     - invalid = an ambiguous match between categories
     - valid = either a category in main which matches exclusively with a
         category in subordinate or an ambiguous match between categories that could
         still be acted upon.

    Example of an ambiguous but valid category match:
        Main category tree:

        A - Product A
            ACA - Company A Product A
                ACARA - Company A Product A Range A
                ACARB - Company A Product A Range B

        Subordinate category tree (where subordinate is only Company A products):

        ACA - Product A
            ACARA - Product A Range A
            ACARB - Product A Range B

        so subordinate ACA matches with main A and ACA after main has been
        processed.
        The category tree can collapse multiple main categories into
        single subordinate.
    """

    if Registrar.DEBUG_CATS:
        Registrar.register_message(
            "matching %d main categories with %d subordinate categories" % (len(
                parsers.main.categories), len(parsers.subordinate.categories)))

    # Matching on "cat_name"

    matches.category = MatchNamespace(
        index_fn=CategoryMatcher.category_index_fn)

    if not (parsers.main.categories and parsers.subordinate.categories):
        return matches

    category_matcher = CategoryMatcher()
    category_matcher.clear()
    category_matcher.process_registers(parsers.subordinate.categories,
                                       parsers.main.categories)

    matches.category.globals.add_matches(category_matcher.pure_matches)
    matches.category.mainless.add_matches(
        category_matcher.mainless_matches)
    matches.category.subordinateless.add_matches(category_matcher.subordinateless_matches)

    if Registrar.DEBUG_CATS:
        if category_matcher.pure_matches:
            Registrar.register_message(
                "All Category matches on cat_name:\n%s" % ('\n'.join(
                    map(str, category_matcher.matches))))

    # using valid because the category tree can collapse multiple
    # main categories into single subordinate

    matches.category.valid += category_matcher.pure_matches

    if category_matcher.duplicate_matches:
        matches.category.duplicate['title'] = \
            category_matcher.duplicate_matches

        for match in category_matcher.duplicate_matches:
            main_taxo_sums = [cat.namesum for cat in match.m_objects]
            # If there is more than one main category in the match,
            # it is only valid if they have the same name
            if (len(main_taxo_sums) > 1 and all(main_taxo_sums)
                    and SeqUtils.check_equal(main_taxo_sums)):
                if len(match.s_objects) == 1:
                    if len(match.m_objects) > 1:
                        deepest_m_object = sorted(
                            [(m_object.depth, m_object)
                             for m_object in match.m_objects])[-1][1]
                        # Other matches are irrelevant if they have the same
                        # name
                        match = Match([deepest_m_object], match.s_objects)
                        # other_match = Match(other_m_objects)
                        # matches.category.subordinateless.append(other_match)
                    matches.category.valid.append(match)
                    continue
                if len(match.s_objects) == 0:
                    matches.category.subordinateless.append(match)
                    continue
            matches.category.invalid.append(match)
        if matches.category.invalid:
            exc = UserWarning("categories couldn't be synchronized because of "
                              "ambiguous names:\n%s" % '\n'.join(
                                  map(str, matches.category.invalid)))
            Registrar.register_error(exc)
            raise exc

    if not (matches.category.subordinateless and matches.category.mainless):
        return matches

    # TODO: Now try and match the mainless / subordinateless categories on title
    # instead of cat_name
    main_orphaned_categories = OrderedDict(
        [(category.rowcount, category) for category in itertools.chain(
            *[match.m_objects for match in matches.category.subordinateless])])
    subordinate_orphaned_categories = OrderedDict(
        [(category.title, category) for category in itertools.chain(
            *[match.s_objects for match in matches.category.mainless])])

    title_matcher = CategoryTitleMatcher()
    title_matcher.clear()
    title_matcher.process_registers(subordinate_orphaned_categories,
                                    main_orphaned_categories)

    matches.category.globals.add_matches(title_matcher.pure_matches)
    matches.category.valid += title_matcher.pure_matches
    matches.category.mainless = MatchList(title_matcher.mainless_matches)
    matches.category.subordinateless = MatchList(title_matcher.subordinateless_matches)

    if matches.category.subordinateless and matches.category.mainless:
        exc = UserWarning(
            "You may want to fix up the following "
            "categories before syncing:\n%s\n%s" % ('\n'.join(
                map(str, category_matcher.subordinateless_matches)), '\n'.join(
                    map(str, category_matcher.mainless_matches))))

        Registrar.register_error(exc)
        # raise exc

    return matches


# TODO: do_match_attributes ?
def do_match_attributes(parsers, matches, settings):
    if settings.do_attributes:
        raise NotImplementedError("Do Match Attributes not implemented")


def do_match_prod(parsers, matches, settings):
    """For every item in subordinate, find its counterpart in main."""

    Registrar.register_progress("Attempting matching")

    if not settings.do_sync:
        return matches

    product_matcher = ProductMatcher()
    product_matcher.process_registers(parsers.subordinate.products,
                                      parsers.main.products)
    # print product_matcher.__repr__()

    matches.globals.add_matches(product_matcher.pure_matches)
    matches.mainless.add_matches(product_matcher.mainless_matches)
    matches.deny_anomalous('product_matcher.mainless_matches',
                           product_matcher.mainless_matches)
    matches.subordinateless.add_matches(product_matcher.subordinateless_matches)
    matches.deny_anomalous('product_matcher.subordinateless_matches',
                           product_matcher.subordinateless_matches)

    try:
        matches.deny_anomalous('product_matcher.duplicate_matches',
                               product_matcher.duplicate_matches, True)
    except AssertionError as exc:
        exc = UserWarning(
            "products couldn't be synchronized because of ambiguous SKUs:%s" %
            '\n'.join(map(str, product_matcher.duplicate_matches)))
        Registrar.register_error(exc)
        raise exc


def do_match_var(parsers, matches, settings):
    # TODO: finish and test
    matches.variation = MatchNamespace(
        index_fn=ProductMatcher.product_index_fn)

    if not settings.do_variations:
        return

    variation_matcher = VariationMatcher()
    variation_matcher.process_registers(parsers.subordinate.variations,
                                        parsers.main.variations)

    if Registrar.DEBUG_VARS:
        Registrar.register_message(
            "variation matcher:\n%s" % variation_matcher.__repr__())

    matches.variation.globals.add_matches(variation_matcher.pure_matches)
    matches.variation.mainless.add_matches(
        variation_matcher.mainless_matches)
    matches.variation.deny_anomalous('variation_matcher.mainless_matches',
                                     variation_matcher.mainless_matches)
    matches.variation.subordinateless.add_matches(
        variation_matcher.subordinateless_matches)
    matches.variation.deny_anomalous('variation_matcher.subordinateless_matches',
                                     variation_matcher.subordinateless_matches)
    if variation_matcher.duplicate_matches:
        matches.variation.duplicate['index'] = \
            variation_matcher.duplicate_matches


def do_merge_images(matches, parsers, updates, settings):
    if not getattr(updates, 'image', None):
        updates.image = UpdateNamespace()

    if not hasattr(matches, 'image'):
        return updates

    sync_handles = settings.sync_handles_img

    for match in matches.image.valid:
        m_object = match.m_object
        for s_object in match.s_objects:

            sync_update = settings.syncupdate_class_img(m_object, s_object)

            sync_update.update(sync_handles)

            if sync_update.m_updated:
                updates.image.main.append(sync_update)

            if not sync_update.important_static:
                updates.image.problematic.append(sync_update)
                continue

            if sync_update.s_updated:
                updates.image.subordinate.append(sync_update)

    for count, match in enumerate(matches.image.subordinateless):
        m_object = match.m_object
        Registrar.register_message(
            "will create image %d: %s" % (count, m_object.identifier))
        if not (m_object.attaches.products or m_object.attaches.categories):
            continue

        empty_s_object = parsers.subordinate.get_empty_attachment_instance()
        sync_update = settings.syncupdate_class_img(m_object, empty_s_object)
        sync_update.update(sync_handles)
        updates.image.subordinateless.append(sync_update)

    # TODO: only delete duplicate images without attaches
    # import pudb; pudb.set_trace()

    for count, match in enumerate(
        # matches.image.duplicate['file_basename']
        matches.image.mainless
    ):
        if not match.s_len:
            continue
        for dup_count, s_object in enumerate(match.s_objects):
            if s_object.attaches:
                Registrar.register_message(
                    "will not delete image with attaches: %d:\n%s\n%s" %
                    (count, s_object.identifier, s_object.attaches))
                continue

            if not dup_count:
                # keep at least 1 of the duplicates
                continue
            sync_update = settings.syncupdate_class_img(None, s_object)
            Registrar.register_message(
                "will delete image: %d:\n%s" % (count, s_object.identifier))

            updates.image.mainless.append(sync_update)

    return updates


def get_update_cat(settings, m_object, s_object):
    sync_update = settings.syncupdate_class_cat(m_object, s_object)
    sync_update.update(settings.sync_handles_cat)

    if settings.do_images:
        sync_update.simplify_sync_warning_value_singular(
            'image', ['id', 'title', 'source_url'])

    return sync_update


def do_merge_categories(matches, parsers, updates, settings):
    if not hasattr(updates, 'category'):
        updates.category = UpdateNamespace()

    if not hasattr(matches, 'category'):
        return updates

    sync_handles = settings.sync_handles_cat

    for match in matches.category.valid:
        s_object = match.s_object
        for m_object in match.m_objects:

            sync_update = get_update_cat(settings, m_object, s_object)
            sync_update = settings.syncupdate_class_cat(m_object, s_object)

            sync_update.update(sync_handles)

            if settings.do_images:
                sync_update.simplify_sync_warning_value_singular(
                    'image', ['id', 'title', 'source_url'])

            if sync_update.m_updated:
                updates.category.main.append(sync_update)

            if not sync_update.important_static:
                updates.category.problematic(sync_update)
                continue

            if sync_update.s_updated:
                updates.category.subordinate.append(sync_update)

    if settings.auto_create_new:
        for count, match in enumerate(matches.category.subordinateless):
            # not all mainless matches have a singular main object.
            # Only select the deepest one.
            if len(match.m_objects) > 1:
                m_object = sorted([(m_object.depth, m_object)
                                   for m_object in match.m_objects])[-1][1]
            else:
                m_object = match.m_object

            # TODO: if there is a pending change to change a subordinate category to
            # the same category name as one being created, then there will be a
            # conflict
            Registrar.register_message(
                "will create category %d: %s" % (count, m_object.identifier))
            empty_s_object = parsers.subordinate.get_empty_category_instance()
            sync_update = settings.syncupdate_class_cat(
                m_object, empty_s_object)
            sync_update.update(sync_handles)
            updates.category.subordinateless.append(sync_update)

    return updates


def do_merge_attributes(matches, parsers, updates, settings):
    if settings.do_attributes:
        raise NotImplementedError("Do Merge Attributes not implemented")


def get_update_prod(settings, m_object, s_object):
    """Return a description of updates required to merge these products."""
    sync_update = settings.syncupdate_class_prod(m_object, s_object)
    sync_update.update(settings.sync_handles_prod)

    if settings.do_categories:
        sync_update.simplify_sync_warning_value_listed('product_categories',
                                                       ['term_id'])

    if settings.do_images:
        sync_update.simplify_sync_warning_value_listed(
            'attachment_objects', ['id', 'title', 'source_url', 'position'])

    if settings.do_attributes:
        sync_update.simplify_sync_warning_value_listed('attributes',
                                                       ['term_id'])

    return sync_update


def do_merge_prod(matches, parsers, updates, settings):
    """Return a description of updates required to merge `matches`."""
    if not settings.do_sync:
        return

    for count, match in enumerate(matches.globals):
        if Registrar.DEBUG_UPDATE:
            Registrar.register_message(
                "processing match %d:\n%s" % (count, match.tabulate()))
        m_object = match.m_object
        s_object = match.s_object

        # , "gcs %s is not variation but object is" % repr(gcs)
        assert not m_object.is_variation
        # , "gcs %s is not variation but object is" % repr(gcs)
        assert not s_object.is_variation

        if m_object.get('post_status') == 'trash':
            matches.mainless.append(match)
            continue

        sync_update = get_update_prod(settings, m_object, s_object)

        # Assumes that GDrive is read only, doesn't care about main
        #   updates

        if Registrar.DEBUG_VARS:
            Registrar.register_message(
                "prod update %d:\n%s" % (count, sync_update.tabulate()))

        if sync_update.m_updated:
            updates.main.append(sync_update)

        if not sync_update.s_updated:
            continue

        if Registrar.DEBUG_UPDATE:
            Registrar.register_message(
                "update %d:\n%s" % (count, sync_update.tabulate()))

        if sync_update.s_updated and sync_update.s_deltas:
            updates.delta_subordinate.append(sync_update)

        if not sync_update.important_static:
            updates.problematic.append(sync_update)
            continue

        if sync_update.s_updated:
            updates.subordinate.append(sync_update)

    for count, match in enumerate(matches.subordinateless):
        m_object = match.m_object

        if m_object.get('post_status') == 'trash':
            continue

        Registrar.register_message(
            "will create product %d: %s" % (count, m_object.identifier))
        sync_update = get_update_prod(settings, m_object, None)
        updates.subordinateless.append(sync_update)

    for count, match in enumerate(matches.mainless):
        s_object = match.s_object

        Registrar.register_message(
            "will delete product: %d:\n%s" % (count, s_object.identifier))

        sync_update = get_update_prod(settings, None, s_object)

        updates.mainless.append(sync_update)


def get_update_var(settings, m_object, s_object):
    sync_update = settings.syncupdate_class_var(m_object, s_object)
    sync_update.update(settings.sync_handles_var)

    if settings.do_images:
        sync_update.simplify_sync_warning_value_singular(
            'image', ['id', 'title', 'source_url'])

    if settings.do_attributes:
        sync_update.simplify_sync_warning_value_listed('attributes',
                                                       ['term_id'])

    return sync_update


def do_merge_var(matches, parsers, updates, settings):
    if not (settings.do_variations and settings.do_sync):
        return

    if not hasattr(updates, 'variation'):
        updates.variation = UpdateNamespace()

    if matches.variation.duplicate:
        exc = UserWarning(
            "variations couldn't be synchronized because of ambiguous SKUs:%s"
            % '\n'.join(map(str, matches.variation.duplicate)))
        Registrar.register_error(exc)
        raise exc

    for count, match in enumerate(matches.variation.globals):
        if Registrar.DEBUG_VARS:
            Registrar.register_message(
                "processing match %d:\n%s" % (count, match.tabulate()))
        m_object = match.m_object
        s_object = match.s_object

        if m_object.get('post_status') == 'trash':
            matches.variation.mainless.append(match)
            continue

        sync_update = get_update_var(settings, m_object, s_object)

        # Assumes that GDrive is read only, doesn't care about main
        #   updates

        if Registrar.DEBUG_VARS:
            Registrar.register_message(
                "var update %d:\n%s" % (count, sync_update.tabulate()))

        if sync_update.s_updated and sync_update.s_deltas:
            updates.variation.delta_subordinate.append(sync_update)

        if sync_update.m_updated:
            updates.variation.main.append(sync_update)

        if not sync_update.important_static:
            updates.variation.problematic.append(sync_update)
            continue

        if sync_update.s_updated:
            updates.variation.subordinate.append(sync_update)

    for count, match in enumerate(matches.variation.subordinateless):
        m_object = match.m_object

        if m_object.get('post_status') == 'trash':
            continue

        sync_update = get_update_var(settings, m_object, None)

        Registrar.register_message(
            "Will create variation %d:\n%s" % (count, m_object.identifier))

        updates.variation.subordinateless.append(sync_update)
        # TODO: figure out which attribute terms to add to parent?

    for count, match in enumerate(matches.variation.mainless):
        s_object = match.s_object

        Registrar.register_message(
            "will delete variation: %d:\n%s" % (count, s_object.identifier))

        sync_update = get_update_var(settings, None, s_object)

        updates.variation.mainless.append(sync_update)
        # TODO: figure out which attribute terms to delete from parent?


def do_report_images(reporters, matches, updates, parsers, settings):
    if not settings.get('do_report'):
        return reporters

    Registrar.register_progress("Write Images Report")

    do_img_sync_group(reporters.img, matches, updates, parsers, settings)

    if settings.get('report_matching'):
        do_matches_group(reporters.img, matches.image, updates, parsers,
                         settings)

    if reporters.img:
        reporters.img.write_document_to_file('img', settings.rep_img_path)

    return reporters


def do_report_categories(reporters, matches, updates, parsers, settings):
    if not settings.get('do_report'):
        return reporters

    Registrar.register_progress("Write Categories Report")

    do_cat_sync_gruop(reporters.cat, matches, updates, parsers, settings)

    if reporters.cat:
        reporters.cat.write_document_to_file('cat', settings.rep_cat_path)

    return reporters


# TODO: do_report_attributes ?


def do_report_attributes(reporters, matches, updates, parsers, settings):
    if settings.do_attributes:
        raise NotImplementedError("Do Report Attributes not implemented")


def do_report(reporters, matches, updates, parsers, settings):
    """Write report of changes to be made."""

    if not settings.get('do_report'):
        return reporters

    Registrar.register_progress("Write Report")

    do_main_summary_group(reporters.main, matches, updates, parsers, settings)
    do_delta_group(reporters.main, matches, updates, parsers, settings)
    do_sync_group(reporters.main, matches, updates, parsers, settings)
    do_var_sync_group(reporters.main, matches, updates, parsers, settings)

    if reporters.main:
        reporters.main.write_document_to_file('main', settings.rep_main_path)

    if settings.get('report_matching'):
        Registrar.register_progress("Write Matching Report")

        do_matches_summary_group(reporters.match, matches, updates, parsers,
                                 settings)
        do_matches_group(reporters.match, matches, updates, parsers, settings)
        if settings.do_variations:
            do_variation_matches_group(reporters.match, matches, updates,
                                       parsers, settings)
        if settings.do_categories:
            do_category_matches_group(reporters.match, matches, updates,
                                      parsers, settings)

        if reporters.match:
            reporters.match.write_document_to_file('match',
                                                   settings.rep_match_path)

    return reporters


def do_report_post(reporters, results, settings):
    """Reports results from performing updates."""
    # raise NotImplementedError()
    if settings.get('do_report'):
        Registrar.register_progress("Write Post Report")

        do_post_summary_group(reporters.post, results, settings)
        do_failures_group(reporters.post, results, settings)
        do_successes_group(reporters.post, results, settings)
        if reporters.post:
            reporters.post.write_document_to_file('post',
                                                  settings.rep_post_path)


def handle_failed_update(update, results, exc, settings, source=None):
    """Handle a failed update."""
    fail = (update, exc)
    if source == settings.main_name:
        pkey = update.main_id
        results.fails_main.append(fail)
    elif source == settings.subordinate_name:
        pkey = update.subordinate_id
        results.fails_subordinate.append(fail)
    else:
        pkey = ''
    Registrar.register_error("ERROR UPDATING %s (%s): %s\n%s\n%s" %
                             (source or '', pkey, repr(exc), update.tabulate(),
                              traceback.format_exc()))

    if Registrar.DEBUG_TRACE:
        import pudb
        pudb.set_trace()


def usr_prompt_continue(settings):
    # TODO: this is completely broken. just read from stdin
    if not settings.ask_before_update:
        return None
    try:
        raw_in = input("\n".join([
            "Please read reports and then make your selection",
            " - press Enter to continue and perform updates",
            " - press s to skip updates", " - press c to cancel", "..."
        ]))
    except SyntaxError:
        raw_in = ""
    if raw_in == 's':
        return 's'
    if raw_in == 'c':
        raise SystemExit
    return None


def upload_new_items_subordinate(parsers,
                           results,
                           settings,
                           client,
                           new_updates,
                           _type='product'):

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(new_updates), items_plural='new %s(s)' % _type)

    if not (new_updates and settings.update_subordinate):
        return

    update_count = 0

    type_get_update_fns = {
        'category': get_update_cat,
        'product': get_update_prod,
        'variation': get_update_var
    }

    type_analyse_api_obj_fns = {
        'image': 'analyse_api_image_raw',
        'category': 'process_api_category_raw',
        'product': 'analyse_api_obj',
        # TODO: fill in this
        'variation': 'process_api_variation_raw'
    }

    while new_updates:

        sync_update = new_updates.pop(0)

        if _type == "category":
            # make sure parent updates are done before children
            new_object_gen = sync_update.old_m_object_gen

            if new_object_gen.parent:
                remaining_m_objects = set(
                    [sync_update.old_m_object_gen for update_ in new_updates])
                parent = new_object_gen.parent
                if not parent.is_root and parent in remaining_m_objects:
                    new_updates.append(sync_update)
                    continue

        if _type in type_get_update_fns:
            # have to refresh sync_update to get parent wpid since parente
            # wpid is populated in do_updates_categories_main
            sync_update = type_get_update_fns.get(_type)(
                settings, sync_update.old_m_object, sync_update.old_s_object)

        core_data = sync_update.get_subordinate_updates()

        # ensure all attachment objects have a valid id
        if 'attachment_objects' in core_data:
            safe_imgs = []
            for img_obj in core_data['attachment_objects']:
                if not img_obj.get('id'):
                    exc = UserWarning(
                        "all attachment objects require a"
                        "valid attachment id")
                    Registrar.register_warning(exc)
                    continue
                safe_imgs += img_obj
            core_data['attachment_objects'] = safe_imgs

        if 'image' in core_data:
            if not core_data['image'].get('id'):
                exc = UserWarning(
                    "attachment object requires a valid attachment id")
                Registrar.register_warning(exc)
                del core_data['image']

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "new %s (core format) %s" % (_type, core_data))

        update_count += 1
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(update_count)

        create_item_kwargs = {}
        process_item_kwargs = {}
        if _type == 'variation':
            parent_id = sync_update.old_m_object.parent['ID']
            create_item_kwargs['parent_pkey'] = parent_id
            process_item_kwargs['parent_id'] = parent_id

        try:
            response = client.create_item_core(core_data, **create_item_kwargs)
            response_api_data = response.json()
        except BaseException as exc:
            handle_failed_update(sync_update, results, exc, settings,
                                 settings.subordinate_name)
            continue
        if client.page_nesting:
            response_api_data = response_api_data[client.endpoint_singular]

        response_gen_object = getattr(
            parsers.subordinate, type_analyse_api_obj_fns[_type])(
                response_api_data, **process_item_kwargs)

        sync_update.set_new_s_object_gen(response_gen_object)
        sync_update.old_m_object_gen.update(response_gen_object)
        results.successes.append(sync_update)


# TODO: collapse upload_new functions
def upload_new_images_subordinate(parsers, results, settings, client, new_updates):

    # TODO: fix constantly re-uploading images

    upload_new_items_subordinate(
        parsers, results, settings, client, new_updates, _type="image")


# TODO: collapse upload_changes functions
def upload_image_changes_subordinate(parsers, results, settings, client,
                               change_updates):
    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(change_updates),
            items_plural='%s updates' % client.endpoint_singular)

    if not settings.update_subordinate:
        return

    for count, sync_update in enumerate(change_updates):
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(count)

        if not sync_update.s_updated:
            continue

        try:
            pkey = sync_update.subordinate_id
            changes = sync_update.get_subordinate_updates()
            response_raw = client.upload_changes_core(pkey, changes)
            response_api_data = response_raw.json()
            if client.page_nesting:
                response_api_data = response_api_data[client.endpoint_singular]
        except Exception as exc:
            handle_failed_update(sync_update, results, exc, settings,
                                 settings.subordinate_name)
            continue

        if response_api_data['id'] != pkey:
            # if pkey has changed since update, i.e. a new item was uploaded
            response_gen_object = parsers.subordinate.analyse_api_image_raw(
                response_api_data)
        else:
            response_core_data = \
                settings.coldata_class_img.translate_data_from(
                    response_api_data, settings.coldata_img_target
                )
            response_gen_data = settings.coldata_class_img.translate_data_to(
                response_core_data, settings.coldata_gen_target_write)
            sync_update.old_s_object_gen.update(response_gen_data)
            response_gen_object = sync_update.old_s_object_gen

        sync_update.set_new_s_object_gen(response_gen_object)
        sync_update.old_m_object_gen.update(response_gen_object)

        results.successes.append(sync_update)


def do_updates_images_main(updates, parsers, results, settings):
    for update in updates.image.main:
        old_main_id = update.main_id
        if Registrar.DEBUG_UPDATE:
            Registrar.register_message(
                "performing update < %5s | %5s > = \n%100s, %100s " %
                (update.main_id, update.subordinate_id, str(update.old_m_object),
                 str(update.old_s_object)))
        if old_main_id not in parsers.main.attachments:
            exc = UserWarning(
                "couldn't fine pkey %s in parsers.main.attachments" %
                update.main_id)
            Registrar.register_error(exc)
            continue
        parsers.main.attachments[old_main_id].update(
            update.get_main_updates_native())


def delete_images_subordinate(parsers, results, settings, client, delete_updates):
    """
    It's not wise to delete images:
        - They could be about to be attached to a product, and it would be
            difficult to tell
        - It could be used in a post without being attached
    """
    pass

    # for update in delete_updates:
    #     client.delete_item(
    #         pkey=update.s_object.wpid
    #     )
    #
    # raise NotImplementedError()


def do_updates_images_subordinate(updates, parsers, results, settings):
    """Perform a list of updates on attachments."""

    results.image = ResultsNamespace()
    results.image.new = ResultsNamespace()

    sync_client_class = settings.subordinate_img_sync_client_class
    sync_client_args = settings.subordinate_img_sync_client_args

    try:
        endpoint_singular = sync_client_class.endpoint_singular
        assert isinstance(endpoint_singular, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_singular = "image"

    try:
        endpoint_plural = sync_client_class.endpoint_plural
        assert isinstance(endpoint_plural, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_plural = "%s" % endpoint_singular

    # updates in which an item is modified
    change_updates = updates.image.subordinate
    if settings.do_problematic:
        change_updates += updates.image.problematic
    # updates in which a new item is created
    new_updates = []
    if settings.auto_create_new:
        new_updates += updates.image.subordinateless
    else:
        for update in updates.image.subordinateless:
            new_item_api = update.get_subordinate_updates_native()
            exc = UserWarning("{0} needs to be created: {1}".format(
                endpoint_singular, new_item_api))
            Registrar.register_warning(exc)

    delete_updates = []
    if settings.auto_delete_old:
        delete_updates += updates.image.mainless
    else:
        for update in updates.image.mainless:
            exc = UserWarning("{} needs to be deleted: {} | {}".format(
                endpoint_singular, update.old_s_object.get('ID'),
                update.old_s_object.get('source_url')))
            Registrar.register_warning(exc)

    Registrar.register_progress(
        "Changing {1}, creating {2} and deleting {3} {0}".format(
            endpoint_plural, len(change_updates), len(new_updates),
            len(delete_updates)))

    if not (new_updates or change_updates or delete_updates):
        return

    if settings['ask_before_update']:
        if usr_prompt_continue(settings) == 's':
            return

    with sync_client_class(**sync_client_args) as client:
        if new_updates:
            upload_new_images_subordinate(parsers, results.image.new, settings,
                                    client, new_updates)

        if change_updates:
            upload_image_changes_subordinate(parsers, results.image, settings,
                                       client, change_updates)

        if delete_updates:
            delete_images_subordinate(parsers, results, settings, client,
                                delete_updates)


def upload_new_categories_subordinate(parsers, results, settings, client,
                                new_updates):
    """
    Create new categories in client in an order which creates parents first.
    """
    upload_new_items_subordinate(
        parsers, results, settings, client, new_updates, _type="category")


def upload_category_changes_subordinate(parsers, results, settings, client,
                                  change_updates):
    """
    Upload a list of category changes
    """

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(change_updates),
            items_plural='%s updates' % client.endpoint_singular)

    if not settings.update_subordinate:
        return

    for count, sync_update in enumerate(change_updates):
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(count)

        if not sync_update.s_updated:
            continue

        try:
            pkey = sync_update.subordinate_id
            changes = sync_update.get_subordinate_updates_native()
            response_raw = client.upload_changes(pkey, changes)
            response_api_data = response_raw.json()
        except Exception as exc:
            handle_failed_update(sync_update, results, exc, settings,
                                 settings.subordinate_name)
            continue

        response_core_data = settings.coldata_class_cat.translate_data_from(
            response_api_data, settings.coldata_cat_target)
        response_gen_data = settings.coldata_class_cat.translate_data_to(
            response_core_data, settings.coldata_gen_target_write)

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "%s being updated with parser data: %s" %
                (client.endpoint_singular, pformat(response_gen_data)))

        sync_update.old_s_object_gen.update(response_gen_data)
        sync_update.set_new_s_object_gen(sync_update.old_s_object_gen)
        sync_update.old_m_object_gen.update(response_gen_data)

        results.successes.append(sync_update)


def do_updates_categories_main(updates, parsers, results, settings):
    for update in updates.category.main:
        if Registrar.DEBUG_UPDATE:
            Registrar.register_message(
                "performing update < %5s | %5s > = \n%100s, %100s " %
                (update.main_id, update.subordinate_id, str(update.old_m_object),
                 str(update.old_s_object)))
        if update.main_id not in parsers.main.categories:
            exc = UserWarning(
                "couldn't fine pkey %s in parsers.main.categories" %
                update.main_id)
            Registrar.register_error(exc)
            continue
        parsers.main.categories[update.main_id].update(
            update.get_main_updates_native())


def delete_categories_subordinate(parsers, results, settings, client,
                            delete_updates):
    raise NotImplementedError()


def do_updates_categories_subordinate(updates, parsers, results, settings):
    """Perform a list of updates on categories."""
    if not hasattr(updates, 'category'):
        return

    results.category = ResultsNamespace()
    results.category.new = ResultsNamespace()

    sync_client_class = settings.subordinate_cat_sync_client_class
    sync_client_args = settings.subordinate_cat_sync_client_args

    try:
        endpoint_singular = sync_client_class.endpoint_singular
        assert isinstance(endpoint_singular, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_singular = "product"

    try:
        endpoint_plural = sync_client_class.endpoint_plural
        assert isinstance(endpoint_plural, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_plural = "%s" % endpoint_singular

    change_updates = updates.category.subordinate
    if settings.do_problematic:
        change_updates += updates.category.problematic
    # updates in which a new item is created
    new_updates = []
    if settings.auto_create_new:
        new_updates += updates.category.subordinateless
    else:
        for update in updates.category.subordinateless:
            new_item_api = update.get_subordinate_updates_native()
            exc = UserWarning("{0} needs to be created: {1}".format(
                endpoint_singular, new_item_api))
            Registrar.register_warning(exc)

    delete_updates = []
    if settings.auto_delete_old:
        delete_updates += updates.category.mainless
    else:
        for update in updates.category.mainless:
            deleted_item_api = update.get_subordinate_updates_native()
            exc = UserWarning("{0} needs to be deleted: {1}".format(
                endpoint_singular, deleted_item_api))
            Registrar.register_warning(exc)

    Registrar.register_progress(
        "Changing {1}, creating {2} and deleting {3} {0}".format(
            endpoint_plural, len(change_updates), len(new_updates),
            len(delete_updates)))

    if not (new_updates or change_updates or delete_updates):
        return

    if settings['ask_before_update']:
        if usr_prompt_continue(settings) == 's':
            return

    with sync_client_class(**sync_client_args) as client:
        if new_updates:
            upload_new_categories_subordinate(parsers, results.category.new,
                                        settings, client, new_updates)

        if change_updates:
            upload_category_changes_subordinate(parsers, results.category, settings,
                                          client, change_updates)

        if delete_updates:
            delete_categories_subordinate(parsers, results, settings, client,
                                    delete_updates)


# TODO: do_updates_attributes_main ?
def do_updates_attributes_main(updates, parsers, results, settings):
    if settings.do_attributes:
        raise NotImplementedError(
            "Do Updates Attributes Main not implemented")


# TODO: do_updates_attributes_subordinate ?
def do_updates_attributes_subordinate(updates, parsers, results, settings):
    if settings.do_attributes:
        raise NotImplementedError(
            "Do Updates Attributes Subordinate not implemented")


def upload_new_products(parsers, results, settings, client, new_updates):
    """
    Create new products in client in an order which creates parents first.
    """

    upload_new_items_subordinate(
        parsers, results, settings, client, new_updates, _type="product")


def upload_product_changes(parsers, results, settings, client, change_updates):

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(change_updates),
            items_plural='%s updates' % client.endpoint_singular)

    if not settings.update_subordinate:
        return

    for count, sync_update in enumerate(change_updates):
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(count)

        if not sync_update.s_updated:
            continue

        try:
            pkey = sync_update.subordinate_id
            changes = sync_update.get_subordinate_updates_native()

            # ensure all attachment objects have a valid id
            if 'images' in changes:
                safe_imgs = []
                for img_obj in changes['images']:
                    if not img_obj.get('id'):
                        exc = UserWarning(
                            "all attachment objects require a"
                            "valid attachment id")
                        Registrar.register_warning(exc)
                        continue
                    safe_imgs += img_obj
                changes['images'] = safe_imgs

            response_raw = client.upload_changes(pkey, changes)
            response_api_data = response_raw.json()
        except Exception as exc:
            handle_failed_update(sync_update, results, exc, settings,
                                 settings.subordinate_name)
            continue

        response_core_data = settings.coldata_class.translate_data_from(
            response_api_data, settings.coldata_cat_target)
        response_gen_data = settings.coldata_class.translate_data_to(
            response_core_data, settings.coldata_gen_target_write)

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "%s being updated with parser data: %s" %
                (client.endpoint_singular, pformat(response_gen_data)))

        sync_update.old_s_object_gen.update(response_gen_data)
        sync_update.set_new_s_object_gen(sync_update.old_s_object_gen)
        sync_update.old_m_object_gen.update(response_gen_data)

        results.successes.append(sync_update)


def delete_products_subordinate(parsers, results, settings, client, delete_updates):
    raise NotImplementedError()


def do_updates_prod_subordinate(updates, parsers, results, settings):
    """
    Update products in subordinate.
    """
    # updates in which an item is modified

    results.new = ResultsNamespace()
    sync_client_class = settings.subordinate_upload_client_class
    sync_client_args = settings.subordinate_upload_client_args

    try:
        endpoint_singular = sync_client_class.endpoint_singular
        assert isinstance(endpoint_singular, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_singular = "product"

    try:
        endpoint_plural = sync_client_class.endpoint_plural
        assert isinstance(endpoint_plural, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_plural = "%s" % endpoint_singular

    change_updates = updates.subordinate
    if settings.do_problematic:
        change_updates += updates.problematic
    # updates in which a new item is created
    new_updates = []
    if settings.auto_create_new:
        new_updates += updates.subordinateless
    else:
        for update in updates.subordinateless:
            new_item_api = update.get_subordinate_updates_native()
            exc = UserWarning("{0} needs to be created: {1}".format(
                endpoint_singular, new_item_api))
            Registrar.register_warning(exc)

    delete_updates = []
    if settings.auto_delete_old:
        delete_updates += updates.mainless
    else:
        for update in updates.mainless:
            deleted_item_api = update.get_subordinate_updates_native()
            exc = UserWarning("{0} needs to be deleted: {1}".format(
                endpoint_singular, deleted_item_api))
            Registrar.register_warning(exc)

    Registrar.register_progress(
        "Changing {1}, creating {2} and deleting {3} {0}".format(
            endpoint_plural, len(change_updates), len(new_updates),
            len(delete_updates)))

    if not (new_updates or change_updates or delete_updates):
        return

    if settings['ask_before_update']:
        if usr_prompt_continue(settings) == 's':
            return

    with sync_client_class(**sync_client_args) as client:
        if new_updates:
            upload_new_products(parsers, results, settings, client,
                                new_updates)
        if change_updates:
            upload_product_changes(parsers, results, settings, client,
                                   change_updates)

        if delete_updates:
            delete_products_subordinate(parsers, results, settings, client,
                                  delete_updates)


def do_updates_prod_main(updates, parsers, settings, results):
    for update in updates.main:
        old_main_id = update.main_id
        if Registrar.DEBUG_UPDATE:
            Registrar.register_message(
                "performing update < %5s | %5s > = \n%100s, %100s " %
                (update.main_id, update.subordinate_id, str(update.old_m_object),
                 str(update.old_s_object)))
        if old_main_id not in parsers.main.items:
            exc = UserWarning("couldn't fine pkey %s in parsers.main.items" %
                              update.main_id)
            Registrar.register_error(exc)
            continue
        parsers.main.items[old_main_id].update(
            update.get_main_updates_native())


def do_updates_var_main(updates, parsers, results, settings):
    for update in updates.variation.main:
        old_main_id = update.main_id
        if Registrar.DEBUG_UPDATE:
            Registrar.register_message(
                "performing update < %5s | %5s > = \n%100s, %100s " %
                (update.main_id, update.subordinate_id, str(update.old_m_object),
                 str(update.old_s_object)))
        if old_main_id not in parsers.main.variations:
            exc = UserWarning(
                "couldn't fine pkey %s in parsers.main.attachments" %
                update.main_id)
            Registrar.register_error(exc)
            continue
        parsers.main.variations[old_main_id].update(
            update.get_main_updates_native())


def upload_new_variations_subordinate(parsers, results, settings, client,
                                new_updates):
    upload_new_items_subordinate(
        parsers, results, settings, client, new_updates, _type="variation")


def upload_variation_changes_subordinate(parsers, results, settings, client,
                                   change_updates):
    """
    Upload a list of variation changes
    """
    try:
        endpoint_singular = client.endpoint_singular
        assert isinstance(endpoint_singular, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_singular = "variation"

    try:
        endpoint_plural = client.endpoint_plural
        assert isinstance(endpoint_plural, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_plural = "%ss" % endpoint_singular

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(change_updates), items_plural='%s updates' % endpoint_singular)

    if not settings.update_subordinate:
        return

    for count, sync_update in enumerate(change_updates):
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(count)

        if not sync_update.s_updated:
            continue

        try:
            pkey = sync_update.subordinate_id
            parent_pkey = sync_update.new_s_object['parent_id']
            changes = sync_update.get_subordinate_updates_native()

            if 'image' in changes:
                if not changes['image'].get('id'):
                    exc = UserWarning(
                        "attachment object requires a valid attachment id")
                    Registrar.register_warning(exc)
                    del changes['image']

            response_raw = client.upload_changes(
                pkey, changes, parent_pkey=parent_pkey)
            response_api_data = response_raw.json()
        except Exception as exc:
            handle_failed_update(sync_update, results, exc, settings,
                                 settings.subordinate_name)
            continue

        response_core_data = settings.coldata_class_cat.translate_data_from(
            response_api_data, settings.coldata_cat_target)
        response_gen_data = settings.coldata_class_cat.translate_data_to(
            response_core_data, settings.coldata_gen_target_write)

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "%s being updated with parser data: %s" %
                (endpoint_singular, pformat(response_gen_data)))

        sync_update.old_s_object_gen.update(response_gen_data)
        sync_update.set_new_s_object_gen(sync_update.old_s_object_gen)
        sync_update.old_m_object_gen.update(response_gen_data)

        results.successes.append(sync_update)


def delete_variations_subordinate(parsers, results, settings, client,
                            delete_updates):
    raise NotImplementedError()


def do_updates_var_subordinate(updates, parsers, results, settings):
    results.variation = ResultsNamespace()
    results.variation.new = ResultsNamespace()

    sync_client_class = settings.subordinate_var_sync_client_class
    sync_client_args = settings.subordinate_var_sync_client_args

    try:
        endpoint_singular = sync_client_class.endpoint_singular
        assert isinstance(endpoint_singular, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_singular = "variation"

    try:
        endpoint_plural = sync_client_class.endpoint_plural
        assert isinstance(endpoint_plural, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_plural = "%ss" % endpoint_singular

    change_updates = updates.variation.subordinate
    if settings.do_problematic:
        change_updates += updates.variation.problematic

    new_updates = []
    if settings.auto_create_new:
        new_updates += updates.variation.subordinateless
    else:
        for update in updates.variation.subordinateless:
            new_item_api = update.get_subordinate_updates_native()
            exc = UserWarning("{0} needs to be created: {1}".format(
                endpoint_singular, new_item_api))
            Registrar.register_warning(exc)

    delete_updates = []
    if settings.auto_delete_old:
        delete_updates += updates.variation.mainless
    else:
        for update in updates.variation.mainless:
            deleted_item_api = update.get_subordinate_updates_native()
            exc = UserWarning("{0} needs to be deleted: {1}".format(
                endpoint_singular, deleted_item_api))
            Registrar.register_warning(exc)

    Registrar.register_progress(
        "Changing {1}, creating {2} and deleting {3} {0}".format(
            endpoint_plural, len(change_updates), len(new_updates),
            len(delete_updates)))

    if not (new_updates or change_updates or delete_updates):
        return

    if settings['ask_before_update']:
        if usr_prompt_continue(settings) == 's':
            return

    with sync_client_class(**sync_client_args) as client:
        if new_updates:
            upload_new_variations_subordinate(parsers, results.variation.new,
                                        settings, client, new_updates)

        if change_updates:
            upload_variation_changes_subordinate(parsers, results.variation,
                                           settings, client, change_updates)

        if delete_updates:
            delete_variations_subordinate(parsers, results, settings, client,
                                    delete_updates)


def main(override_args=None, settings=None):
    """Main function for generator."""
    if not settings:
        settings = SettingsNamespaceProd()
    settings.init_settings(override_args)

    settings.init_dirs()

    ########################################
    # Create Product Parser object
    ########################################

    parsers = ParserNamespace()
    populate_main_parsers(parsers, settings)

    check_warnings(settings)

    if settings.schema_is_woo and settings.do_images:
        process_images(settings, parsers)

    if parsers.main.objects and settings.do_export_main:
        export_main_parser(settings, parsers)

    if settings.main_and_quit:
        sys.exit(ExitStatus.success)

    populate_subordinate_parsers(parsers, settings)

    if parsers.subordinate.objects:
        cache_api_data(settings, parsers)

    matches = MatchNamespace(index_fn=ProductMatcher.product_index_fn)
    updates = UpdateNamespace()
    reporters = ReporterNamespace()
    results = ResultsNamespace()

    if settings.do_images:
        do_match_images(parsers, matches, settings)
        do_merge_images(matches, parsers, updates, settings)
        do_report_images(reporters, matches, updates, parsers, settings)
        Registrar.register_message(
            "pre-sync summary: \n%s" % reporters.img.get_summary_text())
        check_warnings(settings)
        if not settings.report_and_quit:
            do_updates_images_main(updates, parsers, results, settings)
            try:
                do_updates_images_subordinate(updates, parsers, results, settings)
            except (SystemExit, KeyboardInterrupt) as exc:
                Registrar.register_error(exc)
                return reporters, results

    if settings.images_and_quit:
        sys.exit(ExitStatus.success)

    if settings.do_categories:

        do_match_categories(parsers, matches, settings)
        do_merge_categories(matches, parsers, updates, settings)
        do_report_categories(reporters, matches, updates, parsers, settings)
        Registrar.register_message(
            "pre-sync summary: \n%s" % reporters.cat.get_summary_text())
        check_warnings(settings)
        if not settings.report_and_quit:
            do_updates_categories_main(updates, parsers, results, settings)
            if settings.report_matched_categories:
                export_categories(settings, parsers.main,
                                  settings.rep_matched_cat_path, 'wc-csv')
                reporters.cat.add_csv_file('matched_cat',
                                           settings.rep_matched_cat_path)
            try:
                do_updates_categories_subordinate(updates, parsers, results,
                                            settings)
            except (SystemExit, KeyboardInterrupt) as exc:
                Registrar.register_error(exc)
                return reporters, results

    if settings.categories_and_quit:
        sys.exit(ExitStatus.success)

    if settings.do_attributes:
        Registrar.register_error(
            NotImplementedError(
                "Functions past this point have not been completed"))
        return reporters, results

        do_match_attributes(parsers, matches, settings)
        do_merge_attributes(matches, parsers, updates, settings)
        do_report_attributes(reporters, matches, updates, parsers, settings)
        Registrar.register_message(
            "pre-sync summary: \n%s" % reporters.attr.get_summary_text())
        check_warnings(settings)
        if not settings.report_and_quit:
            do_updates_attributes_main(updates, parsers, results, settings)
            try:
                do_updates_attributes_subordinate(updates, parsers, results,
                                            settings)
            except (SystemExit, KeyboardInterrupt) as exc:
                Registrar.register_error(exc)
                return reporters, results

    # product variation IDs must be known before prod sync but any non-existent
    # variable products must be created before their variations can be created.
    if settings.do_variations:
        do_match_var(parsers, matches, settings)
        do_merge_var(matches, parsers, updates, settings)
        # do_report_variations(
        #     reporters, matches, updates, parsers, settings
        # )
        Registrar.register_message(
            "pre-sync summary: \n%s" % reporters.var.get_summary_text())
        check_warnings(settings)
        do_updates_var_main(updates, parsers, settings, results)

    do_match_prod(parsers, matches, settings)
    do_merge_prod(matches, parsers, updates, settings)

    # check_warnings(settings)
    do_report(reporters, matches, updates, parsers, settings)

    if settings.report_and_quit:
        sys.exit(ExitStatus.success)

    check_warnings(settings)

    Registrar.register_message(
        "pre-sync summary: \n%s" % reporters.main.get_summary_text())

    try:
        do_updates_prod_subordinate(updates, parsers, results, settings)
        do_updates_prod_main(updates, parsers, results, settings)
    except (SystemExit, KeyboardInterrupt) as exc:
        Registrar.register_error(exc)
        return reporters, results

    if settings.do_variations:
        if not settings.report_and_quit:
            try:
                do_updates_var_subordinate(updates, parsers, results, settings)
            except (SystemExit, KeyboardInterrupt) as exc:
                Registrar.register_error(exc)
                return reporters, results

    do_report_post(reporters, results, settings)

    Registrar.register_message(
        "post-sync summary: \n%s" % reporters.post.get_summary_text())

    #########################################
    # Display reports
    #########################################

    Registrar.register_progress("Displaying reports")

    if settings.do_report:
        if settings['rep_web_path']:
            shutil.copyfile(settings.rep_main_path, settings['rep_web_path'])
            if settings['web_browser']:
                os.environ['BROWSER'] = settings['web_browser']
                # print "set browser environ to %s" % repr(web_browser)
            # print "moved file from %s to %s" % (settings.rep_main_path,
            # repWeb_path)

            webbrowser.open(settings['rep_web_link'])
    else:
        print("open this link to view report %s" % settings['rep_web_link'])


def catch_main(override_args=None):
    """Run main within a try statement and attempt to analyse failure."""
    file_path = __file__
    cur_dir = os.getcwd() + '/'
    if file_path.startswith(cur_dir):
        file_path = file_path[len(cur_dir):]
    override_args_repr = ''
    if override_args is not None:
        override_args_repr = ' '.join(override_args)

    full_run_str = "%s %s %s" % (str(sys.executable), str(file_path),
                                 override_args_repr)

    settings = SettingsNamespaceProd()

    status = 0
    try:
        main(settings=settings, override_args=override_args)
    except SystemExit:
        status = ExitStatus.failure
    except KeyboardInterrupt:
        pass
    except BaseException as exc:
        status = 1
        if isinstance(exc, UserWarning):
            status = 65
        elif isinstance(exc, IOError):
            status = 74
            print("cwd: %s" % os.getcwd())
        elif exc.__class__ in [
                "ReadTimeout", "ConnectionError", "ConnectTimeout",
                "ServerNotFoundError"
        ]:
            status = 69  # service unavailable

        if status:
            Registrar.register_error(traceback.format_exc())
            Registrar.raise_exception(exc)

    with io.open(settings.log_path, 'w+', encoding='utf8') as log_file:
        for source, messages in Registrar.get_message_items(1).items():
            print(source)
            log_file.writelines([SanitationUtils.coerce_unicode(source)])
            log_file.writelines([
                SanitationUtils.coerce_unicode(message) for message in messages
            ])
            for message in messages:
                pprint(message, indent=4, width=80, depth=2)

    #########################################
    # zip reports
    #########################################

    files_to_zip = [
        settings.rep_fail_main_csv_path, settings.rep_fail_subordinate_csv_path,
        settings.rep_main_path
    ]

    with zipfile.ZipFile(settings.zip_path, 'w') as zip_file:
        for file_to_zip in files_to_zip:
            try:
                os.stat(file_to_zip)
                zip_file.write(file_to_zip)
            except BaseException:
                pass
        Registrar.register_message('wrote file %s' % zip_file.filename)

    # print "\nexiting with status %s \n" % status
    if status:
        print("re-run with: \n%s" % full_run_str)
    else:
        Registrar.register_message("re-run with:\n%s" % full_run_str)

    sys.exit(status)


if __name__ == '__main__':
    catch_main()
